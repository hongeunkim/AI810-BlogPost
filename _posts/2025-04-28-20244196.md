---
layout: distill
title: Protein Ensemble Generation Paper Review 
description: Diffusion-style generators are now beginning to rival molecular-dynamics (MD) simulations for capturing the conformational landscapes that govern protein function. Jing et al. fine-tune AlphaFold under a flow-matching loss to create “AlphaFlow,” which samples diverse, sequence-conditioned structural ensembles that match MD-derived distributions while operating orders of magnitude faster. Lewis, Jiménez-Luna et al. introduce “BioEmu,” a scalable deep-learning framework that reproduces equilibrium ensembles across thousands of proteins with similarly drastic speed-ups and accurate thermodynamic observables, together signalling a shift from static structure prediction to routine, high-throughput emulation of protein dynamics. 
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# anonymize when submitting 
authors:
  - name: Hong Eun Kim 
    url: "https://www.linkedin.com/in/hongeunkim/"
    affiliations:
      name: KAIST

bibliography: 2025-04-28-20244196.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Protein Ensemble Sampling
  - name: 1. AlphaFlow
  - name: 2. BioEmu
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .mycodeblock {
    color:#429472;
  }
  img.animated-gif{
    height: 250px;
    width:auto;
  }
  a {
    color: #87CEFA;
  }

  /* Optional: Change color on hover */
  a:hover {
      color: #1E90FF;
  }
  .callout-block {
      width: 85%;
      margin: 25px auto;           
      padding: 15px 30px;          
      background-color: #1f2937;
      color: #f3f4f6;
      border-radius: 8px;    
  }

---
# Protein Ensemble Sampling

Understanding and ultimately controlling protein behaviour ranks among the most ambitious goals in modern biomedicine and biotechnology: catalytic efficiency, signalling specificity and even pathogenic mis-folding all derive from a protein’s finely tuned relationship between sequence, structure and dynamics.

Deep-learning breakthroughs such as AlphaFold2 <d-cite key="Jumper2021"> </d-cite> and RoseTTAFold <d-cite key="Baek2021"> </d-cite> have revolutionised the static aspect of that relationship, predicting single high-confidence conformations for entire proteomes with near-experimental accuracy and thereby accelerating everything from structural genomics to in-silico drug screening.

Yet biology happens on an energy landscape, not at a single point: ligand recognition, allostery and post-translational regulation depend on the ability of a polypeptide chain to roam a rugged free-energy surface populated by many metastable states. <d-cite key="ALLISON20201707"> </d-cite>  Long-timescale molecular-dynamics (MD) simulations remain the canonical tool for mapping that surface, but even with specialised hardware (e.g. the Anton supercomputer) micro- to millisecond trajectories of a single protein in explicit water can cost days of dedicated GPU time, and adding realistic partners—small molecules, ions or membranes—exacerbates the sampling burden. <d-cite key="Heilmann2020"> </d-cite>

<br>
{% include figure.html path="assets/img/2025-04-28-20244196/Energy_Landscape.png" class="img-fluid w-75 d-block mx-auto" %}
<div class="caption">
    Image Source: <a href="https://www.nature.com/articles/s41598-017-10525-5" target="_blank">https://www.nature.com/articles/s41598-017-10525-5</a>
</div>

The conceptual picture behind this cost is the funnel-shaped free-energy landscape: proteins fold and function by descending energetic funnels whose rims are littered with kinetic traps; adequately visiting those traps and the transitional saddles between them is what makes exhaustive MD so resource-hungry. <d-cite key="Chong2019"> </d-cite>

Generative modelling offers a data-driven shortcut. Denoising-diffusion and closely related flow-matching frameworks first proved their worth in de novo backbone design (e.g., RFdiffusion), and have now begun to learn full equilibrium ensembles. AlphaFlow <d-cite key="jing2024alphafoldmeetsflowmatching"> </d-cite> fine-tunes AlphaFold under a flow-matching loss to sample sequence-conditioned conformers that quantitatively reproduce MD observables, while BioEmu <d-cite key="Lewis2024.12.05.626885"> </d-cite> leverages diffusion-style training on >200 ms of MD trajectories to generate thousands of independent equilibrium structures per hour on a single GPU. Together with earlier machine-learning studies that directly synthesise conformational ensembles from simulation data, these systems point to a future in which deep generative models act as practical surrogates for physics-based simulations.

---

# 1. AlphaFlow

### Introduction

{% include figure.html path="assets/img/2025-04-28-20244196/Alpha_Flow_Overview.png" class="img-fluid w-90 d-block mx-auto" %}
<div class="caption">
    Image Source: <a href="https://arxiv.org/abs/2402.04845" target="_blank">AlphaFlow</a>
</div>

Proteins rarely operate as rigid macromolecules; instead, they explore rugged free-energy landscapes whose multiple basins encode binding, allostery and catalysis. Capturing these conformational ensembles has long relied on micro- to millisecond molecular-dynamics (MD) trajectories—an enterprise that is accurate but computationally prohibitive when realistic solvent, ions and ligands are included. Recent breakthroughs in static structure prediction—most notably AlphaFold2 and RoseTTAFold—have narrowed the sequence-to-structure gap, yet they output only a single conformation and thus miss the dynamical heterogeneity crucial for function. Bowen Jing et al. bridge this gap by coupling AlphaFold with the flow-matching generative framework, yielding AlphaFlow—a model that learns and samples sequence-conditioned structural ensembles with MD-like fidelity but orders-of-magnitude less compute. 

### Flow Matching

Flow matching seeks a time-dependent **vector field** $$\mathbf{v}_\theta(\mathbf{x},t)$$ that deterministically transports a simple *prior* distribution $$ q(\mathbf{x}) $$ into the *data* distribution $$ p_{\text{data}}(\mathbf{x}) $$.

Given a pair $$\bigl(\mathbf{x}_0,\mathbf{x}_1\bigr)$$ —here, a noisy template and a native structure—the *target* velocity at interpolation time $$ t\in[0,1] $$ is  

$$
\mathbf{v}^{\star}_t\!\bigl(\mathbf{x}_0,\mathbf{x}_1\bigr) = \mathbf{x}_1-\mathbf{x}_0 .
$$

The **flow-matching loss** minimises mean-squared error between modelled and target velocities  

$$
\mathcal{L}_{\text{FM}}(\theta)=
\mathbb{E}_{t\sim\mathcal{U}(0,1)}
\;\mathbb{E}_{(\mathbf{x}_0,\mathbf{x}_1)\sim p_{\text{pair}}}
\Bigl\|
\mathbf{v}_\theta\!\bigl(\mathbf{x}_t,t\bigr)
-
\mathbf{v}^{\star}_t\!\bigl(\mathbf{x}_0,\mathbf{x}_1\bigr)
\Bigr\|_2^{2},
\qquad
$$

$$
\mathbf{x}_t=(1-t)\mathbf{x}_0 + t\mathbf{x}_1 .
$$

At inference, conformations are generated by integrating the neural ODE  

$$
\frac{d\mathbf{x}}{dt} = \mathbf{v}_\theta(\mathbf{x},t), 
\qquad t:0\;\rightarrow\;1,
$$

starting from a sample of the harmonic prior $$ q(\mathbf{x}) $$.  
Flow matching thus generalises score-based diffusion without requiring noise schedules or stochastic denoising steps.

### Method

{% include figure.html path="assets/img/2025-04-28-20244196/AlphaFold_as_Diffusion.png" class="img-fluid w-50 d-block mx-auto" %}
<div class="caption">
    Image Source: <a href="https://arxiv.org/abs/2402.04845" target="_blank">AlphaFlow</a>
</div>

**Goal.** For a protein sequence $A = (a_1,\dots,a_N)$, learn the conditional distribution of backbone coordinates  

$$
p\!\left(\mathbf{x}\mid A\right), \qquad \mathbf{x}\in\mathbb{R}^{3\times N},
$$

whose support is the full conformational ensemble of that sequence.

#### Key insight — “Any regressor can be a denoiser”

AlphaFold is a deterministic map  

$$
f_{\text{AF}}\;:\;A \;\longmapsto\; \hat{\mathbf{x}},
$$

trained with a mean-squared-error (MSE) loss.  
Text-to-image diffusion models show that such a regressor becomes a **denoiser** if, besides the conditioning text, it receives a **noisy** version of the target.  
Applying the same trick converts AlphaFold into a structure denoiser.

#### Architectural recipe

- **Generate a noisy template**
   Sample a perturbed structure $\tilde{\mathbf{x}}$ by adding Gaussian noise or by linearly interpolating between known conformers.

- **Dual input**
   Feed the pair $(A,\tilde{\mathbf{x}})$ into AlphaFold.  
   A thin *template-embedding stack*—adapted from AlphaFold’s own template pipeline—is prepended to the frozen Evoformer trunk so the network can ingest the noisy coordinates.

- **Training objectiv**
   Predict a cleaned structure $\hat{\mathbf{x}}$ and minimise  

   $$
   \mathcal{L}_{\text{denoise}}
     = \| \hat{\mathbf{x}} - \mathbf{x} \|_2^{2},
   $$

   where $\mathbf{x}$ is the ground-truth conformation.

### Experiments and Results 

{% include figure.html path="assets/img/2025-04-28-20244196/6uof_A_animation.gif" class="img-fluid d-block mx-auto w-75" %}

| Task | Baseline | **AlphaFlow** | Metric |
|------|----------|---------------|--------|
| PDB hold-out proteins (multi-state) | AlphaFold + MSA subsampling | ↑ diversity at equal/better TM-score | Precision–Diversity AUC ↑ 16 % |
| MD-derived ensembles (87 proteins) | Replicate MD | **×10** speed-up to RMSF convergence | C-β RMSF RMSE ↓ 23 % |
| Equilibrium observables | MD ground truth | Matches $$R_g$$, distance distributions | KL-divergence \< 0.05 |

**Qualitative findings.** Generated ensembles recover hinge-like motions and loop breathing absent from single-structure predictions. Conditioning on ligand-bound templates lets AlphaFlow populate closed, ligand-competent states even when trained solely on apo structures, hinting at transferable induced-fit knowledge.

**Limitations & outlook.** AlphaFlow inherits AlphaFold’s memory footprint and cannot yet model explicit side-chain rearrangements or solvent coupling. Incorporating SE(3)-equivariant layers or coarse-grained solvent fields could address these gaps. Still, AlphaFlow establishes a compelling paradigm: **leveraging accurate regressors as generative flow predictors** to democratise ensemble sampling once confined to bespoke MD hardware.

---

# 2. BioEmu

### Introduction
Predicting how a protein **moves** rather than how it merely looks is critical for understanding catalysis, allostery and ligand recognition, yet exhaustive molecular-dynamics (MD) simulations that map the Boltzmann distribution,  

$$
p_{\text{eq}}(\mathbf{x}\,|\,A)=\frac{e^{-F(\mathbf{x})/k_{\mathrm B}T}}{Z(A)},
\qquad 
\mathbf{x}\!\in\!\mathbb R^{3\times N},
$$

remain orders-of-magnitude too slow for routine use.  
Lewis *et&nbsp;al.* tackle this bottleneck with **BioEmu**, a generative deep-learning system that draws *thousands* of statistically independent conformations per hour on a single GPU while reproducing key thermodynamic observables across diverse proteins. 

### Model

{% include figure.html path="assets/img/2025-04-28-20244196/BioEmu_Overview.png" class="img-fluid d-block mx-auto" %}
<div class="caption">
    Image Source: <a href="https://www.biorxiv.org/content/10.1101/2024.12.05.626885v1" target="_blank">BioEmu</a>
</div>

BioEmu retains the **overall architecture of Distributional Graphormer** <d-cite key="zheng2023predictingequilibriumdistributionsmolecular"> </d-cite> for coordinate generation, but crucially swaps its original training recipe for a denoising-diffusion approach:

1. **Sequence encoding once per protein**  
   * The input amino-acid sequence is passed through the AlphaFold 2 <d-cite key="Jumper2021"> </d-cite> Evoformer to obtain *single* and *pair* representations $$h_{\text{single}},\,h_{\text{pair}}$$.  
   * These embeddings are computed only once for each protein and reused throughout sampling.

2. **Denoising diffusion backbone**  
   * $$h_{\text{single}},h_{\text{pair}}$$ feed a **denoising diffusion model** that iteratively transforms a noisy coordinate field $$\tilde{\mathbf{x}}^{(t)}$$ into a clean structure $$\mathbf{x}$$.  
   * A **second-order integrator** requires as few as 100 denoising steps, enabling fast inference:  
     $$
       \tilde{\mathbf{x}}^{(t-1)} \;=\; \tilde{\mathbf{x}}^{(t)} - \tfrac{\Delta t}{2}\!\left[
         s_\theta\!\bigl(\tilde{\mathbf{x}}^{(t)},t,h_{\text{single}},h_{\text{pair}}\bigr)\;+\;
         s_\theta\!\bigl(\tilde{\mathbf{x}}^{(t-1)},t-\Delta t,h_{\text{single}},h_{\text{pair}}\bigr)
       \right],
     $$
     where $$s_\theta$$ denotes the score/denoiser network.

3. **Pre-training on AFDB**  
   * A clustered subset of the AlphaFold Database (AFDB) is used, together with a data-augmentation scheme that encourages diverse conformations.

4. **Continued training with heterogeneous data**  
   * Stage 2 mix:  
     - **$$\sim\!200\;\text{ms}$$** of curated all-atom MD trajectories for small/medium proteins, re-weighted toward equilibrium by Markov State Models (MSM or experimental constraints.  
     - A filtered subset of the MEGAscale stability dataset (~10⁶ ΔΔG measurements).  
     - Occasional examples from the AFDB pre-training set to maintain diversity.  
   * Experimental ΔΔG data enter via property-prediction fine-tuning (PPFT), a new algorithm that back-propagates stability errors through the diffusion model without needing explicit structure labels.

5. **Sampling throughput**  
   * With the second-order integrator and sequence features cached, 10 000 statistically independent structures can be drawn within minutes to a few hours on a single GPU, depending on protein length.

6. **Generalisation filters**  
   * The final BioEmu model is evaluated only on test proteins that share ≤ 40 % sequence identity with any training protein of length ≥ 20 residues, ensuring rigorous out-of-distribution assessment.

The name **“BioEmu”** refers to this *fine-tuned* model trained on (i) clustered AFDB, (ii) re-weighted MD ensembles, and (iii) experimental stability measurements.

### Tasks

#### 1. Sampling Conformational Changes

* **Motivation.** To test whether **BioEmu** can reproduce *known* structural rearrangements, the authors introduce two progressively harder benchmarks and compare against **AFCluster <d-cite key="Wayment-Steele2024"> </d-cite>** and **AlphaFlow<d-cite key="jing2024alphafoldmeetsflowmatching"> </d-cite>**.

| Benchmark | Composition | Sequence–identity filter | Key finding |
|-----------|-------------|--------------------------|-------------|
| **OOD60** | 19 proteins exhibiting large‐scale motions (e.g.\ ligand‐induced clamp, dimerisation shifts) | ≤ 60 % to the AlphaFold-monomer model and ≤ 40 % to any BioEmu training sequence | BioEmu recovers the target conformer significantly more often than both baselines. |
| **Extended Motion Set** | ≈ 100 proteins with domain swings, local unfolding, or cryptic‐pocket formation | Mixed — some <40 % ID, some overlapping the AlphaFold-2 training set | BioEmu again outperforms AFCluster and AlphaFlow in every category *except* the apo states of the cryptic‐pocket subset; the margin is largest for proteins outside AlphaFold-2’s training distribution.|

{% include figure.html path="assets/img/2025-04-28-20244196/Conformational_Change.png" class="img-fluid w-80 d-block mx-auto" %}
<div class="caption">
    Image Source: <a href="https://www.biorxiv.org/content/10.1101/2024.12.05.626885v1" target="_blank">BioEmu</a>
</div>

* **Insight.**  Even when restricted to single‐domain information, BioEmu’s denoising trajectories explore hinge closures and loop breathing motions that elude both clustering of AlphaFold models (AFCluster) and the flow-matching approach of AlphaFlow—suggesting that the fine-tuning on re-weighted MD data imparts a broader kinetic repertoire.  Crucially, comparable success rates on proteins that do or do not overlap the AlphaFold-2 training corpus indicate that BioEmu’s multi-conformer capability is **not** a mere artefact of memorised Evoformer embeddings but stems from genuine generative generalisation.

#### 2. Emulating MD Equilibrium Distributions

* **Motivation.**  Exhaustive MD sampling sufficient to map Boltzmann‐weighted conformational landscapes routinely demands $$10^{2}\,\mu\text{s}\!-\!10^{4}\,\text{ms}$$ per target.  Such trajectories are feasible only on special-purpose hardware (e.g.\ Anton) or via massive distributed campaigns stitched together with statistical models.  BioEmu is designed to *sidestep* that sampling bottleneck by learning the equilibrium distribution directly.

{% include figure.html path="assets/img/2025-04-28-20244196/MD_Equilibrium.png" class="img-fluid w-80 d-block mx-auto" %}
<div class="caption">
    Image Source: <a href="https://www.biorxiv.org/content/10.1101/2024.12.05.626885v1" target="_blank">BioEmu</a>
</div>

* **Performances** On the **DESRES fast-folder set** (12 proteins), BioEmu reproduces the 2-D free-energy landscape with a mean absolute error  

$$
\text{MAE}_{F} \approx 0.74\;\text{kcal·mol}^{-1},
$$

matching the accuracy variation across state-of-the-art MD force fields.  

* **Take-away** BioEmu fine-tuned on $$\sim 200\ \text{ms}$$ of heterogeneous MD achieves **sub-kcal mol$$^{-1}$$** accuracy in free-energy landscapes across fast folders, CATH domains and challenging IDPs, while offering **4–5 orders-of-magnitude** speed-ups over conventional simulation.


#### 3. Predicting Protein Stabilities

BioEmu treats stability as a special case of equilibrium sampling: the fraction of **folded** versus **unfolded** conformers in its ensemble should reproduce the experimental folding free-energy  

$$
\Delta G \;=\; G_{\text{folded}} - G_{\text{unfolded}} .
$$

* **Training signal** – > 750 000 ΔΔG measurements from MEGAscale and 25 ms of MD trajectories for 27 k (wild-type + mutant) sequences.  
* **Property-Prediction Fine-Tuning (PPFT)** – a lightweight loop that samples just 8 denoising steps, labels each structure by native-contact fraction, and back-propagates the mismatch between predicted foldedness and experiment.  

{% include figure.html path="assets/img/2025-04-28-20244196/Stabilities.png" class="img-fluid w-80 d-block mx-auto" %}
<div class="caption">
    Image Source: <a href="https://www.biorxiv.org/content/10.1101/2024.12.05.626885v1" target="_blank">BioEmu</a>
</div>

* **Performance** – mean-absolute error < 0.8 kcal mol⁻¹ and Spearman ρ > 0.65 on held-out MEGAscale proteins, surpassing sequence-only baselines.  
* **Generalisation checks** – correctly classifies highly stable ProThermDB proteins (ΔG < –8 kcal mol⁻¹) as folded and produces radius-of-gyration estimates for CALVADOS IDPs that follow experimental trends without any IDP-specific training.  
* **Take-away** – BioEmu links generative sampling directly to thermodynamic observables, providing accurate, physics-consistent ΔG predictions at negligible compute cost.

---

# Conclusion

Deep‐learning generators based on **diffusion** and **flow matching** have shifted the long-standing problem of sampling protein ensembles from GPU-month molecular dynamics to GPU-minute neural integration.  By reframing well-validated regressors (AlphaFold, Distributional Graphormer) as *denoisers*, AlphaFlow and BioEmu learn the conditional distribution  

$$
p(\mathbf{x}\mid A)\;\; \text{or} \;\; p_{\text{eq}}(\mathbf{x}\mid A)
$$

and can draw thousands of near-independent conformers that collectively recover native basins, transient intermediates and even unfolded states—all with errors on the order of a single MD force-field swap.  These results demonstrate that **probabilistic sampling and thermodynamic observables are now within reach of data-driven models**, opening the door to high-throughput studies of allostery, cryptic pocket formation and mutation effects.

Yet the current training regime inherits limitations from the underlying datasets:

* **Time-window bias** Most available MD trajectories span μs–ms, leaving slow collective modes (> ms) under-represented.  
* **System size bias** Benchmarks focus on single domains (< ~300 aa) because long explicit-water simulations of large complexes remain prohibitive.  
* **Force-field artefacts** Re-weighting can mitigate but not eradicate inaccuracies baked into classical potentials.

Addressing these gaps will likely require: (i) expanded *experimental* ensemble data (NMR, cryo-EM variability, HDX-MS), (ii) hierarchical or coarse-grained diffusion models that natively handle >1 M-atom assemblies, and (iii) hybrid training objectives that blend quantum-level energy corrections with learned priors.

Despite these hurdles, the trajectory is clear: **generative protein dynamics is evolving from proof-of-concept to practical tool**, promising rapid exploratory sampling, physics-aware stability prediction and, ultimately, routine in-silico navigation of the protein free-energy landscape.